{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46b293f",
   "metadata": {},
   "source": [
    "This work borrows heavily from the code base at: https://github.com/ccatalao/covid19vaccine-emotions/blob/master/olstorm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04efe018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c83603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>start</th>\n",
       "      <th>five_year_mark</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Konza tech</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eko Atlantic</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Waterfall City</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hope city</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vision city</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          project        start  five_year_mark     end_date\n",
       "0      Konza tech   2013-01-01      2018-01-01   2020-12-31\n",
       "1    Eko Atlantic   2009-01-01      2014-01-01   2020-12-31\n",
       "2  Waterfall City   2006-01-01      2011-01-01   2020-12-31\n",
       "3       Hope city   2013-01-01      2018-01-01   2020-12-31\n",
       "4     Vision city   2013-01-01      2018-01-01   2020-12-31"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_df = pd.read_csv('search_data.csv')\n",
    "search_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3391c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://medium.datadriveninvestor.com/how-to-build-a-twitter-scraping-app-with-python-b3fc069a19c6\n",
    "def create_data_file(idx, first_5=True, maxTweets = 100000):\n",
    "    project, start, five_year_mark, end_date = list(search_df.iloc[idx])\n",
    "    print(project, \"5\" if first_5 else \"\")\n",
    "    \n",
    "    csvFile = open(f\"tweets/{project}{'_5' if first_5 else ''}.csv\", 'a', newline='', encoding='utf8')\n",
    "\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['id','datetime','tweets'])\n",
    "\n",
    "    query = f'{str.lower(project)} lang:en since:{str(start if first_5 else five_year_mark).strip()} until:{str(five_year_mark if first_5 else end_date).strip()}'\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        print('# downloaded tweets:', i, end='\\r')\n",
    "        if i > maxTweets :\n",
    "            break\n",
    "        csvWriter.writerow([str(tweet.id)+\"\", tweet.date, tweet.content])\n",
    "    csvFile.close()\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8b22dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konza tech 5\n",
      "# downloaded tweets: 297\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_composer_source=true&include_ext_alt_text=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=konza+tech+lang%3Aen+since%3A2013-01-01+until%3A2018-01-01&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAgLDZ-uvtixgWgIC3pdet0qUaEnEVpNp5FYCJehgHREVGQVVMVDUBFQQVAAA%3D&pc=1&spelling_corrections=1&ext=mediaStats%252CcameraMoment: ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))), retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# downloaded tweets: 1297\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_composer_source=true&include_ext_alt_text=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=konza+tech+lang%3Aen+since%3A2013-01-01+until%3A2018-01-01&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAwLX9nf_ZrhEWgIC3pdet0qUaEnEVhL54FYCJehgHREVGQVVMVDUBFRgVAAA%3D&pc=1&spelling_corrections=1&ext=mediaStats%252CcameraMoment: ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))), retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donewnloaded tweets: 2255\n",
      "Konza tech \n",
      "donewnloaded tweets: 6238\n",
      "Eko Atlantic 5\n",
      "# downloaded tweets: 4791\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_composer_source=true&include_ext_alt_text=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=eko+atlantic+lang%3Aen+since%3A2009-01-01+until%3A2014-01-01&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAwJPUysX9twgWgMDC-Ime0c0LEnEVjJx0FYCJehgHREVGQVVMVDUBFV4VAAA%3D&pc=1&spelling_corrections=1&ext=mediaStats%252CcameraMoment: ChunkedEncodingError(ProtocolError(\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))), retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donewnloaded tweets: 9130\n",
      "Eko Atlantic \n",
      "# downloaded tweets: 9076\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_composer_source=true&include_ext_alt_text=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=eko+atlantic+lang%3Aen+since%3A2014-01-01+until%3A2020-12-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaCwLTtuY6dviEWjsC4gfPznKglEnEVyP5uFYCJehgHREVGQVVMVDUBFbQBFQAA&pc=1&spelling_corrections=1&ext=mediaStats%252CcameraMoment: ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=10)\")), retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donewnloaded tweets: 47859\n",
      "Waterfall City 5\n",
      "donewnloaded tweets: 1679\n",
      "Waterfall City \n",
      "donewnloaded tweets: 36780\n",
      "Hope city 5\n",
      "# downloaded tweets: 2096\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_composer_source=true&include_ext_alt_text=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=hope+city+lang%3Aen+since%3A2013-01-01+until%3A2018-01-01&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAwLzd0JXdthkWgMC10bvmyqYaEnEVmMF3FYCJehgHREVGQVVMVDUBFSgVAAA%3D&pc=1&spelling_corrections=1&ext=mediaStats%252CcameraMoment: ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))), retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donewnloaded tweets: 63642\n",
      "Hope city \n",
      "donewnloaded tweets: 27027\n",
      "Vision city 5\n",
      "donewnloaded tweets: 3446\n",
      "Vision city \n",
      "# downloaded tweets: 1199\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_composer_source=true&include_ext_alt_text=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=vision+city+lang%3Aen+since%3A2018-01-01+until%3A2020-12-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaWwLv5suiXxx0WiMCx-cS9-6clEnEVrM14FYCJehgHREVGQVVMVDUBFRYVAAA%3D&pc=1&spelling_corrections=1&ext=mediaStats%252CcameraMoment: ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))), retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donewnloaded tweets: 1765\n"
     ]
    }
   ],
   "source": [
    "# maxTweets = 1000\n",
    "\n",
    "for i in range(len(search_df)):\n",
    "    create_data_file(i, True)# first 5\n",
    "    create_data_file(i, False)# after the first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab2d83",
   "metadata": {},
   "source": [
    "Emotions: anger, fear, anticipation, trust, surprise, sadness, joy, disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9f6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1984fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re  # for regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8aaba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPLIST = set(stopwords.words('english'))\n",
    "SYMBOLS = set(\" \".join(string.punctuation).split(\" \") + \\\n",
    "[\"-\", \"...\", \"â€\", \"``\", \",\", \".\", \":\", \"''\",\"#\",\"@\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1b9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NLTK lemmatizer and stemmer classes\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14bfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the POS tagging from NLTK to retain only adjectives, verbs, adverbs \n",
    "# and nouns as a base for for lemmatization.\n",
    "def get_lemmas(tweet): \n",
    "    \n",
    "    # A dictionary to help convert Treebank tags to WordNet\n",
    "    treebank2wordnet = {\n",
    "        'NN':'n', # noun\n",
    "        'JJ':'a', # adjective\n",
    "        'VB':'v', # verb\n",
    "        'RB':'r'  # adverb\n",
    "    }\n",
    "    \n",
    "    postag = ''\n",
    "    lemmas_list = []\n",
    "    \n",
    "    for word, tag in pos_tag(word_tokenize(tweet)):\n",
    "        if tag.startswith(\"JJ\")     \\\n",
    "            or tag.startswith(\"RB\") \\\n",
    "            or tag.startswith(\"VB\") \\\n",
    "            or tag.startswith(\"NN\"):\n",
    "                \n",
    "            try:\n",
    "                postag = treebank2wordnet[tag[:2]] # get the base form of tag\n",
    "            except:\n",
    "                postag = 'n'                \n",
    "                            \n",
    "            lemmas_list.append(lemmatizer.lemmatize(word.lower(), postag))    \n",
    "    \n",
    "    return lemmas_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f619b2",
   "metadata": {},
   "source": [
    "Pre-process the tweets, following a pipeline of tokenization, filtering, case normalization and lemma extraction, including an overall cleaning of html and other codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576f91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to clean and filter the tokens in each tweet\n",
    "def clean_tweet(tokens):\n",
    "    \n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            if token not in STOPLIST:\n",
    "                if token[0] not in SYMBOLS:\n",
    "                    if not token.startswith('http'):\n",
    "                        if  '/' not in token:\n",
    "                            if  '-' not in token:\n",
    "                                filtered.append(token)\n",
    "                                        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee73ffb",
   "metadata": {},
   "source": [
    "Prior to lemmatization, apply POS (part-of-speech) tagging to make sure that only the adjectives, verbs, adverbs and nouns are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572fe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts the lemmatization process\n",
    "def get_lemmatized(tweet):\n",
    "   \n",
    "    all_tokens_string = ''\n",
    "    filtered = []\n",
    "    tokens = []\n",
    "\n",
    "    # lemmatize\n",
    "    tokens = [token for token in get_lemmas(tweet)]\n",
    "    \n",
    "    # filter\n",
    "    filtered = clean_tweet(tokens)\n",
    "\n",
    "    # join everything into a single string\n",
    "    all_tokens_string = ' '.join(filtered)\n",
    "    \n",
    "    return all_tokens_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c48c578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eko Atlantic.csv',\n",
       " 'Eko Atlantic_5.csv',\n",
       " 'Hope city.csv',\n",
       " 'Hope city_5.csv',\n",
       " 'Konza tech.csv',\n",
       " 'Konza tech_5.csv',\n",
       " 'Vision city.csv',\n",
       " 'Vision city_5.csv',\n",
       " 'Waterfall City.csv',\n",
       " 'Waterfall City_5.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file in os.listdir('tweets'):\n",
    "    df = pd.read_csv(f'tweets/{file}')\n",
    "    \n",
    "    # get the lemmatized tweets and puts the result in an \"edited\" text column\n",
    "    # for future use in this script\n",
    "    edited_tweets = []\n",
    "    for tweet in df['tweets']:\n",
    "        edited = get_lemmatized(tweet)\n",
    "        if len(edited) > 0:\n",
    "            edited_tweets.append(edited)\n",
    "        else:\n",
    "            edited_tweets.append(None)\n",
    "    df['edited'] = edited_tweets\n",
    "    \n",
    "    # After lemmatization, some tweets may end up with the same words\n",
    "    # Let's make sure that we have no duplicates\n",
    "    df.drop_duplicates(subset=['edited'], inplace=True)\n",
    "    df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175bc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca71fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fcfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9062647",
   "metadata": {},
   "source": [
    "Vader sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d804f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime']=pd.to_datetime(df['datetime']) \n",
    "df.sort_values('datetime', inplace=True, ascending=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59904ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling VADER\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VADER Compound value for sentiment intensity\n",
    "df['sentiment_intensity'] = [analyzer.polarity_scores(edited_tweet)['compound'] for edited_tweet in df['edited']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de612ff5",
   "metadata": {},
   "source": [
    "The output of VADER are the positive, negative, and neutral ratios of sentiment. The most useful metric in VADER is the Compound score. Basically, it is calculated by a sum of the scores of each word, normalized to yeld values between -1, the most extreme negative score, and +1, the most extreme positive.\n",
    "\n",
    "From this normalized score, I will then create a categorical variable (\"sentiment\"), with an output of positive, negative and neutral ratios of sentiment, using the following thresholds:\n",
    "\n",
    "Positive sentiment : (compound score >= 0.05).\n",
    "\n",
    "Neutral sentiment : (compound score > -0.05) and (compound score < 0.05).\n",
    "\n",
    "Negative sentiment : (compound score <= -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef48394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the sentiment category\n",
    "def get_sentiment(intensity):\n",
    "    if intensity >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif (intensity >= -0.05) and (intensity < 0.05):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Using pandas apply/lambda to speed up the process\n",
    "df['sentiment'] = df.apply(lambda x: get_sentiment(x['sentiment_intensity']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list of the emotions\n",
    "emotions = ['Anger', 'Anticipation','Disgust','Fear', 'Joy','Sadness', 'Surprise', 'Trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data from the NCR lexicon\n",
    "ncr = pd.read_csv('ncr_lexicon.csv', sep =';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cee4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_edited_string(edited_tweets):\n",
    "    \n",
    "    edited_string = ''\n",
    "    for row in edited_tweets:\n",
    "        edited_string = edited_string + ' ' + row\n",
    "        \n",
    "    return edited_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the edited tweets in one single string\n",
    "joined_string = join_edited_string(df['edited'])\n",
    "\n",
    "# Get tokens\n",
    "tokens = joined_string.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build now two dictionaries with indexes and unique words, for future reference\n",
    "\n",
    "unique_words = set(tokens)\n",
    "\n",
    "word_to_ind = dict((word, i) for i, word in enumerate(unique_words))\n",
    "ind_to_word = dict((i, word) for i, word in enumerate(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_emotions(df, emotions, col):\n",
    "\n",
    "    df_tweets = df.copy()\n",
    "#     df_tweets.drop(['sentiment','sentiment_intensity'], axis=1, inplace=True)\n",
    "    \n",
    "    emo_info = {'emotion':'' , 'emo_frq': defaultdict(int) }    \n",
    "\n",
    "    list_emotion_counts = []\n",
    "\n",
    "    # creating a dictionary list to hold the frequency of the words\n",
    "    # contributing to the emotions\n",
    "    for emotion in emotions:\n",
    "        emo_info = {}\n",
    "        emo_info['emotion'] = emotion\n",
    "        emo_info['emo_frq'] = defaultdict(int)\n",
    "        list_emotion_counts.append(emo_info)\n",
    "    \n",
    "    # bulding a zeros matrix to hold the emotions data\n",
    "    df_emotions = pd.DataFrame(0, index=df.index, columns=emotions)\n",
    "\n",
    "    \n",
    "    # stemming the word to facilitate the search in NRC\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    # iterating in the tweets data set\n",
    "    for i, row in df_tweets.iterrows(): # for each tweet ...\n",
    "        tweet = word_tokenize(df_tweets.loc[i][col])\n",
    "        for word in tweet: # for each word ...\n",
    "            word_stemmed = stemmer.stem(word.lower())\n",
    "            # check if the word is in NRC\n",
    "            result = ncr[ncr.English == word_stemmed]\n",
    "            # we have a match\n",
    "            if not result.empty:\n",
    "                # update the tweet-emotions counts\n",
    "                for idx, emotion in enumerate(emotions):\n",
    "                    df_emotions.at[i, emotion] += result[emotion]\n",
    "                    \n",
    "                    # update the frequencies dictionary list\n",
    "                    if result[emotion].any():\n",
    "                        try:\n",
    "                            list_emotion_counts[idx]['emo_frq'][word_to_ind[word]] += 1\n",
    "                        except:\n",
    "                            continue\n",
    "    \n",
    "    # append the emotions matrix to the tweets data set\n",
    "    df_tweets = pd.concat([df_tweets, df_emotions], axis=1)\n",
    "\n",
    "    return df_tweets, list_emotion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf97784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the NCR lexicon to associate words to emotions \n",
    "# Be patient, this will take some time ...\n",
    "\n",
    "df_emo, list_emotion_counts = get_tweet_emotions(df, emotions, 'edited')\n",
    "\n",
    "# Preparing for time series\n",
    "df_emo['datetime']= pd.to_datetime(df_emo['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec2c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_zeros = []\n",
    "for i in range(len(df_emo)):\n",
    "    all_zeros.append(df_emo.Anger[i] == df_emo.Anticipation[i] == df_emo.Disgust[i] == df_emo.Fear[i] == df_emo.Joy[i] == df_emo.Sadness[i] == df_emo.Surprise[i] == df_emo.Trust[i] == 0)\n",
    "\n",
    "df_emo[\"all_zeros\"] = all_zeros\n",
    "\n",
    "# Get indexes where all_zeros column has value True\n",
    "indexNames = df_emo[df_emo['all_zeros'] == True].index # Delete these row indexes from dataFrame\n",
    "df_emo.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce54517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo.reset_index(drop=True, inplace=True)\n",
    "df_emo.drop(columns=['all_zeros'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_counts = {}\n",
    "for emo in df_emo.columns[-8:]:\n",
    "    emo_counts[emo] = df_emo[emo].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9663028",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = list(emo_counts.keys())\n",
    "counts = list(emo_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(emotions, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo['tweets'][4738]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32685f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
